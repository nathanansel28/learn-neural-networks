{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Prove that the transpose of the transpose of a matrix is the matrix itself: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n",
    "1. Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that sum and transposition commute: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$.\n",
    "1. Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Can you prove the result by using only the results of the previous two exercises?\n",
    "1. We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code. \n",
    "1. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?\n",
    "1. Run `A / A.sum(axis=1)` and see what happens. Can you analyze the results?\n",
    "1. When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?\n",
    "1. Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along axes 0, 1, and 2?\n",
    "1. Feed a tensor with three or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?\n",
    "1. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{14}}$, initialized with Gaussian random variables. You want to compute the product $\\mathbf{A} \\mathbf{B} \\mathbf{C}$. Is there any difference in memory footprint and speed, depending on whether you compute $(\\mathbf{A} \\mathbf{B}) \\mathbf{C}$ or $\\mathbf{A} (\\mathbf{B} \\mathbf{C})$. Why?\n",
    "1. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$. Is there any difference in speed depending on whether you compute $\\mathbf{A} \\mathbf{B}$ or $\\mathbf{A} \\mathbf{C}^\\top$? Why? What changes if you initialize $\\mathbf{C} = \\mathbf{B}^\\top$ without cloning memory? Why?\n",
    "1. Consider three matrices, say $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200}$. Construct a tensor with three axes by stacking $[\\mathbf{A}, \\mathbf{B}, \\mathbf{C}]$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $\\mathbf{B}$. Check that your answer is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544],\n",
      "        [0.5695, 0.4388, 0.6387, 0.5247, 0.6826, 0.3051],\n",
      "        [0.4635, 0.4550, 0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.3138, 0.1980, 0.4162, 0.2843, 0.3398, 0.5239],\n",
      "        [0.7981, 0.7718, 0.0112, 0.8100, 0.6397, 0.9743]])\n",
      "tensor([[0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544],\n",
      "        [0.5695, 0.4388, 0.6387, 0.5247, 0.6826, 0.3051],\n",
      "        [0.4635, 0.4550, 0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.3138, 0.1980, 0.4162, 0.2843, 0.3398, 0.5239],\n",
      "        [0.7981, 0.7718, 0.0112, 0.8100, 0.6397, 0.9743]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION 1\n",
    "torch.manual_seed(1)\n",
    "m = torch.randint(1, 10, (1,)).item()  \n",
    "n = torch.randint(1, 10, (1,)).item()  \n",
    "A = torch.rand(m, n)\n",
    "print(A)\n",
    "print((A.T).T)\n",
    "\n",
    "torch.equal(A, (A.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6944],\n",
      "        [1.0402],\n",
      "        [1.2468],\n",
      "        [1.0091],\n",
      "        [1.3514],\n",
      "        [0.6291],\n",
      "        [1.3215]])\n",
      "tensor([[0.6944],\n",
      "        [1.0402],\n",
      "        [1.2468],\n",
      "        [1.0091],\n",
      "        [1.3514],\n",
      "        [0.6291],\n",
      "        [1.3215]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION 2\n",
    "torch.manual_seed(2)\n",
    "m = torch.randint(1, 10, (1,)).item()  \n",
    "n = torch.randint(1, 10, (1,)).item()  \n",
    "A = torch.rand(m, n)\n",
    "B = torch.rand(m, n)\n",
    "print(A.T + B.T)\n",
    "print((A + B).T)\n",
    "\n",
    "torch.equal(A.T + B.T, (A + B).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 3\n",
    "# Symmetric matrix = matrix that is equal to its transpose\n",
    "# Prove that A + A.T = (A + A.T).T\n",
    "\n",
    "torch.manual_seed(3)\n",
    "n = torch.randint(1, 10, (1,)).item()  \n",
    "A = torch.rand(n, n)\n",
    "print(torch.equal(A+A.T, (A+A.T).T))\n",
    "\n",
    "# Using past 2 exercises only:\n",
    "    # (A.T + B.T) = (A + B).T\n",
    "# Let B = A.T \n",
    "    # (A.T + A.T.T) = (A + A.T).T\n",
    "# Since A.T.T = A:\n",
    "    # (A.T + A) = (A + A.T).T\n",
    "    # (A + A.T) = (A + A.T).T (shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION 4\n",
    "# len = 2\n",
    "X = torch.rand(2, 3, 4)\n",
    "X.shape, len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 5\n",
    "# The 0-th axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1056, 0.2858, 0.0270, 0.4716, 0.0601],\n",
      "        [0.7719, 0.7437, 0.5944, 0.8879, 0.4510],\n",
      "        [0.7995, 0.1498, 0.4015, 0.0542, 0.4594],\n",
      "        [0.1756, 0.9492, 0.8473, 0.8749, 0.6483],\n",
      "        [0.2148, 0.9493, 0.0121, 0.1809, 0.1877]])\n",
      "tensor([[0.1111, 0.0829, 0.0145, 0.1349, 0.0389],\n",
      "        [0.8124, 0.2156, 0.3188, 0.2540, 0.2920],\n",
      "        [0.8415, 0.0434, 0.2153, 0.0155, 0.2974],\n",
      "        [0.1848, 0.2752, 0.4545, 0.2503, 0.4197],\n",
      "        [0.2260, 0.2753, 0.0065, 0.0517, 0.1215]])\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 6\n",
    "print(A)\n",
    "print(A / A.sum(axis=1)) #performs broadcasting: each element of row i in A is divided by the sum of elements in row i. The result is a matrix where each row sums to 1, effectively normalizing each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 7\n",
    "# Distance = sum of length and width of manhattan\n",
    "# No diagonal travelling possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6303, 0.8147, 0.1841, 0.0459],\n",
      "         [0.5515, 0.8334, 0.8044, 0.4928],\n",
      "         [0.3666, 0.6594, 0.8477, 0.0693]],\n",
      "\n",
      "        [[0.4527, 0.8780, 0.8852, 0.9630],\n",
      "         [0.6931, 0.9937, 0.0165, 0.5946],\n",
      "         [0.5719, 0.8448, 0.5996, 0.8202]]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[1.0829, 1.6927, 1.0693, 1.0090],\n",
      "        [1.2446, 1.8271, 0.8208, 1.0875],\n",
      "        [0.9385, 1.5042, 1.4472, 0.8895]])\n",
      "torch.Size([3, 4])\n",
      "tensor([[1.5484, 2.3074, 1.8361, 0.6081],\n",
      "        [1.7176, 2.7166, 1.5013, 2.3779]])\n",
      "torch.Size([2, 4])\n",
      "tensor([[1.6750, 2.6821, 1.9430],\n",
      "        [3.1790, 2.2979, 2.8365]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 8\n",
    "X = torch.rand(2, 3, 4)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(X.sum(axis=0))\n",
    "print(X.sum(axis=0).shape)\n",
    "print(X.sum(axis=1))\n",
    "print(X.sum(axis=1).shape)\n",
    "print(X.sum(axis=2))\n",
    "print(X.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5932)\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 9\n",
    "X = torch.rand(2, 3, 4, 5)\n",
    "norm = torch.linalg.norm(X)\n",
    "print(norm) \n",
    "# Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 10\n",
    "# Calculating AB first results in a matrix of size 2^10 x 2^5\n",
    "# Thus to calculate (AB)C we then calculate a 2^10 x 2^5 matrix with a 2^5 x 2^14 matrix\n",
    "\n",
    "# Calculating BC first results in a matrix of size 2^5 x 2^14 (more then when calculating AB first)\n",
    "# Thus to calculate A(BC) we then calculate a 2^5 x 2^14 matrix with a 2^14 x 2^16 matrix\n",
    "\n",
    "# Thus (AB)C is faster and less memory-intensive than A(BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 11\n",
    "# AB is straightforward and efficient \n",
    "# AC.T involves transposing, which introduces overhead, and the multiplication is less cache-efficient\n",
    "# If we initialize C=B.T then we will perform AC.T faster as no additional memory is made for the transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100, 200])\n",
      "tensor([[-0.1947, -1.4064,  0.7817,  ..., -0.8303, -0.2056, -0.0417],\n",
      "        [ 0.0946,  0.1747, -0.2156,  ...,  1.4695, -0.2440,  1.3265],\n",
      "        [-1.1548,  1.0822,  0.3364,  ..., -0.4410, -2.6759, -1.0082],\n",
      "        ...,\n",
      "        [ 0.5379, -0.0781,  0.3327,  ..., -0.2385,  0.2094, -0.5183],\n",
      "        [ 1.2246, -0.6464, -0.0763,  ..., -0.4052,  0.5520, -0.8774],\n",
      "        [ 1.5563,  1.0051, -0.1847,  ...,  0.8655,  0.4168, -0.2630]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 12\n",
    "A = torch.randn((100,200))\n",
    "B = torch.randn((100,200))\n",
    "C = torch.randn((100,200))\n",
    "ABC = torch.stack([A, B, C])\n",
    "print(ABC.shape)\n",
    "print(ABC[1])\n",
    "print(torch.equal(B, ABC[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
